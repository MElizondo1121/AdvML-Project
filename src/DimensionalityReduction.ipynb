{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f439f67",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ebfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_processed.csv', header=0)\n",
    "test = pd.read_csv('../data/test_processed.csv', header=0)\n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train\n",
    "pca = PCA().fit(data)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de410a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(data)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data, n):\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    data_normalized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "    cov_matrix = np.cov(data_normalized.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
    "    eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    k = 115\n",
    "    top_k_eigenvectors = np.array([eigen_pair[1] for eigen_pair in eigen_pairs[:k]])\n",
    "\n",
    "    # Project the original data onto the new subspace\n",
    "    data_pca = np.dot(data_normalized, top_k_eigenvectors.T)\n",
    "    return data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ae934",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['Machine failure'], axis=1)\n",
    "y = train['Machine failure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e65a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ff63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=125)  # Choose the number of components\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98378ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions = rfc.predict(X_test_pca) \n",
    "print(classification_report(y_val, rfc_predictions))\n",
    "print(confusion_matrix(y_val, rfc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(rfc, n_features_to_select=100)\n",
    "rfe.fit(X_train_pca, y_train)\n",
    "selected_featuresRFE_reg = X.columns[rfe.support_]\n",
    "print('Selected Features:', len(selected_featuresRFE_reg))\n",
    "print(selected_featuresRFE_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LightGBM parameters\n",
    "hyper  = {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300, 'num_leaves': 50}\n",
    "\n",
    "# Create and fit a LightGBM model\n",
    "light = lgb.LGBMClassifier(**hyper)\n",
    "light.fit(X_train_pca, y_train)\n",
    "\n",
    "# Feature selection using SelectFromModel\n",
    "threshold = 'median'\n",
    "feature_selector = SelectFromModel(light, threshold=threshold)\n",
    "model = feature_selector.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "X_train_selected = model.transform(X_train_pca)\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = light.feature_importances_\n",
    "feature_names = X.columns\n",
    "# Create a DataFrame for feature importances\n",
    "print(feature_names, feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3117488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
